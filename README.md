# ğŸ¦ Bankruptcy Prediction Hackathon

This repository contains the complete workflow for predicting corporate bankruptcy using financial indicators.  
Developed for a machine learning hackathon, organized by **Department of Management Studies (DMS), Indian Institutes of Technology (IIT) (ISM), Dhanbad**. This project focuses on maximizing **Macro-F1 Score** (primary metric) and **Recall on the â€œfailedâ€ class** (tie-breaker metric).

#### About the hackathon: 
 - Name: *Predict2Protect*
 - Predict2Protect is a week-long data-driven hackathon under PLUTUSâ€™25, where participants build machine learning models to predict corporate bankruptcy using real-world financial datasets.
 - The competition aims to evaluate end-to-end data science skills â€” from data preprocessing and feature engineering to model optimization and business interpretation.
---

## ğŸ“Š Project Overview

Financial distress prediction is a crucial task in corporate finance and risk management.  
Given anonymized financial data of companies over multiple years, the goal is to classify each company as either:

- **`alive`** â€” company remains solvent  
- **`failed`** â€” company declared bankruptcy  

---

## ğŸ“ Repository Structure

```
ğŸ“¦ bankruptcy-prediction
â”‚
â”œâ”€â”€ ğŸ““ bankruptcy-eda-model.ipynb # Complete EDA + preprocessing + model training
â”œâ”€â”€ ğŸ“„ train.csv # Training dataset (not uploaded if private)
â”œâ”€â”€ ğŸ“„ test.csv # Test dataset (not uploaded if private)
â”œâ”€â”€ ğŸ“„ submission_final_hackathon.csv # Final submission file (alive/failed)
â”œâ”€â”€ ğŸ“„ report.tex # LaTeX report file
â”œâ”€â”€ ğŸ“„ README.md # Project documentation (this file)
â””â”€â”€ ğŸ“‚ figures/ # Optional: plots (EDA, threshold tuning, etc.)
```


---

## âš™ï¸ Features and Methods

### ğŸ§¹ **1. Data Preprocessing**
- Handled missing values and outliers (1stâ€“99th percentile clipping)
- Log-transformation on skewed ratios
- Standard scaling using `StandardScaler`
- Encoded target labels: `alive â†’ 0`, `failed â†’ 1`

### ğŸ“ˆ **2. Feature Engineering**
Derived six key financial ratios from the masked variables:

| Ratio | Formula |
|:------|:---------|
| **Debt Ratio** | X17 / X10 |
| **Current Ratio** | X1 / X14 |
| **Profit Margin** | X6 / X16 |
| **Return on Assets (ROA)** | X6 / X10 |
| **Asset Turnover** | X16 / X10 |
| **Inventory Turnover** | X2 / X5 |

Additionally, log-transformed and scaled variants of these ratios were included.

### ğŸ¤– **3. Model Development**
Two main models were tested:
- **Random Forest + SMOTE (baseline)**
- **XGBoost (final model)**

The final XGBoost model used:
```python
n_estimators = 400
learning_rate = 0.05
max_depth = 6
subsample = 0.8
colsample_bytree = 0.8
scale_pos_weight = ratio(negatives/positives)
```

---

## âš–ï¸ 4. Handling Class Imbalance

Used `scale_pos_weight` in `XGBoost` instead of `SMOTE` for better generalization on imbalanced data.

---

## ğŸ¯ 5. Threshold Optimization

Rather than using the default 0.5 cutoff, precisionâ€“recall analysis was used to maximize Macro-F1:

| Threshold | Precision | Recall | F1        |
| --------- | --------- | ------ | --------- |
| **0.343** | 0.258     | 0.431  | **0.323** |

---

## ğŸ“Š Model Evaluation (Validation)

| Metric    | Class 0 (Alive) | Class 1 (Failed) | Macro Avg |
| :-------- | :-------------- | :--------------- | :-------- |
| Precision | 0.957           | 0.258            | 0.607     |
| Recall    | 0.911           | 0.431            | 0.671     |
| F1-Score  | 0.934           | 0.323            | 0.628     |

 - Final Threshold: 0.343
 - Macro-F1: â‰ˆ 0.63
 - Recall (Failed): â‰ˆ 0.43

---

## ğŸ§© Final Submission Format

Required format for hackathon upload:

```
company_name	status_label
C_1	alive
C_2	failed
C_3	alive
```

Generated by:

```python
label_map = {0: "alive", 1: "failed"}
submission['status_label'] = submission['Prediction'].map(label_map)
submission[['company_name', 'status_label']].to_csv("submission_final_hackathon.csv", index=False)
```

---

## ğŸ§  Key Learnings

 - Importance of domain-inspired feature engineering (ratios improved recall significantly).
 - Threshold tuning is crucial when F1 is the target metric.
 - XGBoostâ€™s scale_pos_weight handled class imbalance better than SMOTE in this case.
 - Cross-validation + precision-recall curves lead to robust threshold selection.

---

ğŸš€ Future Improvements

 - Hyperparameter tuning with Optuna (`scoring='f1_macro'`)
 - Ensemble stacking with LightGBM and CatBoost
 - Probability calibration (Platt or Isotonic)
 - SHAP analysis for feature interpretability

---

## ğŸ“š Tech Stack

 - Language: Python (3.12)
 - Libraries: pandas, numpy, matplotlib, seaborn, scikit-learn, xgboost
 - Notebook: Google Colab / Jupyter
 - Report: LaTeX (Overleaf)
 - Version Control: Git + GitHub

---

ğŸ“ References

 - Kaggle: Credit Risk Modelling / Bankruptcy Datasets
 - XGBoost Documentation
 - scikit-learn API Reference

---

### ğŸ‘¤ Author

 - Sukrat Singh, Engineering Student, IIT Dhanbad
 - ğŸ“§ Contact: [email](24je0702@iitism.ac.in)
